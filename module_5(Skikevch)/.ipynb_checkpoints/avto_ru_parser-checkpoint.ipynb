{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "685a72bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass: 1; Page: 6; 212 links\n",
      "D:\\Work\\DS_Projects\\module_6(Skikevch)\\Links.txt\n"
     ]
    }
   ],
   "source": [
    "# собираем ссылки на страницы авто\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "\n",
    "HEADERS = {\n",
    "    'user-agent': 'Chrome/85.0.4183.121',\n",
    "    'accept': '*/*'\n",
    "    }\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Получаем html код страницы.\n",
    "    \"\"\"\n",
    "    html = requests.get(url, headers=HEADERS)\n",
    "    html.encoding = 'utf8'\n",
    "    return html\n",
    "\n",
    "def get_pages_count(text_html):\n",
    "    \"\"\"\n",
    "    Выясняем максимальное число страниц.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text_html, 'html.parser')\n",
    "    pagination = soup.find_all('span',class_='Button__text')\n",
    "    if pagination:\n",
    "        return int(pagination[-4].get_text())\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def write_file(links_list, file_name='Links.txt'):\n",
    "    \"\"\"\n",
    "    Запись ссылок в файл построчно\n",
    "    \"\"\"\n",
    "    path = os.path.join(os.path.abspath('.'), file_name)\n",
    "    with open(path, 'a', newline = '\\r',encoding='utf8') as f:\n",
    "        f.writelines('%s\\n' % s for s in links_list)\n",
    "    return path\n",
    "        \n",
    "def result_status(html):\n",
    "    \"\"\"\n",
    "    Проверяем на корректрость обработки get-запроса\n",
    "    \"\"\"\n",
    "    status = html.status_code\n",
    "    if status == 200:\n",
    "        return 1\n",
    "    elif status == 500:\n",
    "        print(f'ERROR. To many requests.'\n",
    "              f'Status code: {html.status_code}')\n",
    "    return 0\n",
    "\n",
    "def get_ulrs_list(yaer_from=None, yaer_to=None):\n",
    "    \"\"\"\n",
    "    Получаем список всех ссылок на авто.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        links = []\n",
    "        # Формируем ссылку на главную страницу каталога\n",
    "        url = 'https://auto.ru/moskva/cars/all/'\n",
    "        if yaer_from and yaer_to:\n",
    "            url += f'?year_from={yaer_from}&year_to={yaer_to}'\n",
    "        elif yaer_from:\n",
    "            url += f'?year_from={yaer_from}'\n",
    "        elif yaer_to:\n",
    "            url += f'?year_to={yaer_to}'    \n",
    "        # Общее число ссылок\n",
    "        c = 0\n",
    "        passage = 1\n",
    "        # Проходим по всем страницам\n",
    "        while True:\n",
    "            # Получаем html код страницы\n",
    "            html = get_html(url)\n",
    "            if not result_status(html):\n",
    "                return\n",
    "            # Получаем всех страниц в на главной каталога\n",
    "            count_pages = get_pages_count(html.text)\n",
    "            page = 1\n",
    "            error_iter = 0\n",
    "            # цикл по всем страницам на главной каталолга\n",
    "            while (page < count_pages+1):\n",
    "                time.sleep(2)\n",
    "                page_url = url\n",
    "                # дополняем url информацией о страницу\n",
    "                if page_url[-1] != '/':\n",
    "                    page_url += f'&page={page}'\n",
    "                else:\n",
    "                    page_url += f'?page={page}'\n",
    "                # получаем html код \n",
    "                html = get_html(page_url)\n",
    "                # проверяем статус ответа\n",
    "                if not result_status(html):\n",
    "                    error_iter += 1\n",
    "                    path = write_file(links)\n",
    "                    time.sleep(5)\n",
    "                    if error_iter == 2:\n",
    "                        page += 1\n",
    "                        error_iter = 0\n",
    "                    continue\n",
    "                # Парсим\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "                # Получаем список объектов, содержащих ссылку\n",
    "                items = soup.find_all('a',class_='Link OfferThumb')\n",
    "                # Проходим по всем объектам и заносим ссылку в список\n",
    "                for item in items:\n",
    "                    links.append(item.get('href'))\n",
    "                # Обновляем число ссылок\n",
    "                c += len(items)\n",
    "                # Выводи отладочную информацию\n",
    "                clear_output()\n",
    "                print(f'Pass: {passage}; Page: {page}; {c} links')\n",
    "                page += 1\n",
    "            # Сохраняем данные в файл\n",
    "            path = write_file(links)\n",
    "            # Обнуляем список чтобы не занимать оперативку,\n",
    "            # всё равно ссылки уже лежат в файле\n",
    "            links = []\n",
    "            # Точка выхода\n",
    "            if c > 2000000:\n",
    "                break\n",
    "            passage += 1\n",
    "        print(f'File path: {path}')\n",
    "        return path\n",
    "    except KeyboardInterrupt:\n",
    "        print(write_file(links))\n",
    "        \n",
    "        \n",
    "get_ulrs_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf0fd6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Audi3146', 'BMW7794', 'Hyundai5944', 'Kia7021', 'LADA (ВАЗ)6444', 'Mercedes-Benz9387', 'Mitsubishi3030', 'Nissan4190', 'Renault4447', 'Toyota3769', 'Volkswagen5185']\n"
     ]
    }
   ],
   "source": [
    "# собираем ссылки на страницы авто\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "\n",
    "HEADERS = {\n",
    "    'user-agent': 'Chrome/85.0.4183.121',\n",
    "    'accept': '*/*'\n",
    "    }\n",
    "\n",
    "def get_html(url):\n",
    "    \"\"\"\n",
    "    Получаем html код страницы.\n",
    "    \"\"\"\n",
    "    html = requests.get(url, headers=HEADERS)\n",
    "    html.encoding = 'utf8'\n",
    "    return html\n",
    "\n",
    "def get_pages_count(text_html):\n",
    "    \"\"\"\n",
    "    Выясняем максимальное число страниц.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(text_html, 'html.parser')\n",
    "    pagination = soup.find_all('span',class_='Button__text')\n",
    "    if pagination:\n",
    "        return int(pagination[-4].get_text())\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_models_list(text_html):\n",
    "    models_list = []\n",
    "    soup = BeautifulSoup(text_html, 'html.parser')\n",
    "    models = soup.find_all('div',class_='ListingPopularMMM__item')\n",
    "    for model in models:\n",
    "        models_list.append(model.text)\n",
    "    print(models_list)\n",
    "\n",
    "def write_file(links_list, file_name='Links.txt'):\n",
    "    \"\"\"\n",
    "    Запись ссылок в файл построчно\n",
    "    \"\"\"\n",
    "    path = os.path.join(os.path.abspath('.'), file_name)\n",
    "    with open(path, 'a', newline = '\\r',encoding='utf8') as f:\n",
    "        f.writelines('%s\\n' % s for s in links_list)\n",
    "    return path\n",
    "        \n",
    "def result_status(html):\n",
    "    \"\"\"\n",
    "    Проверяем на корректрость обработки get-запроса\n",
    "    \"\"\"\n",
    "    if html.status_code != 200:\n",
    "        \n",
    "        print(f'ERROR. Status code: {html.status_code}')\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def get_ulrs_list(yaer_from=None, yaer_to=None):\n",
    "    \"\"\"\n",
    "    Получаем список всех ссылок на авто.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        links = []\n",
    "        # Формируем ссылку на главную страницу каталога\n",
    "        url = 'https://auto.ru/moskva/cars/all/'\n",
    "        if yaer_from and yaer_to:\n",
    "            url += f'?year_from={yaer_from}&year_to={yaer_to}'\n",
    "        elif yaer_from:\n",
    "            url += f'?year_from={yaer_from}'\n",
    "        elif yaer_to:\n",
    "            url += f'?year_to={yaer_to}'    \n",
    "        # Общее число ссылок\n",
    "        c = 0\n",
    "        passage = 1\n",
    "        # Проходим по всем страницам\n",
    "        while True:\n",
    "            # Получаем html код страницы\n",
    "            html = get_html(url)\n",
    "            if not result_status(html):\n",
    "                return\n",
    "            # Получаем всех страниц в на главной каталога\n",
    "            count_pages = get_pages_count(html.text)\n",
    "            page = 1\n",
    "            # цикл по всем страницам на главной каталолга\n",
    "            while (page < count_pages+1):\n",
    "                error_iter = 0\n",
    "                page_url = url\n",
    "                # дополняем url информацией о страницу\n",
    "                if page_url[-1] != '/':\n",
    "                    page_url += f'&page={page}'\n",
    "                else:\n",
    "                    page_url += f'?page={page}'\n",
    "                # получаем html код \n",
    "                html = get_html(page_url)\n",
    "                # проверяем статус ответа\n",
    "                if not result_status(html):\n",
    "                    error_iter += 1\n",
    "                    path = write_file(links)\n",
    "                    time.sleep(0.34)\n",
    "                    if error_iter == 3:\n",
    "                        page += 1\n",
    "                        error_iter = 0\n",
    "                    continue\n",
    "                # Парсим\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "                # Получаем список объектов, содержащих ссылку\n",
    "                items = soup.find_all('a',class_='Link OfferThumb')\n",
    "                # Проходим по всем объектам и заносим ссылку в список\n",
    "                for item in items:\n",
    "                    links.append(item.get('href'))\n",
    "                # Обновляем число ссылок\n",
    "                c += len(items)\n",
    "                # Выводи отладочную информацию\n",
    "                clear_output()\n",
    "                print(f'Pass: {passage}; Page: {page}; {c} links')\n",
    "                page += 1\n",
    "            # Сохраняем данные в файл\n",
    "            path = write_file(links)\n",
    "            # Обнуляем список чтобы не занимать оперативку,\n",
    "            # всё равно ссылки уже лежат в файле\n",
    "            links = []\n",
    "            # Точка выхода\n",
    "            if c > 2000000:\n",
    "                break\n",
    "            passage += 1\n",
    "        print(f'File path: {path}')\n",
    "        return path\n",
    "    except KeyboardInterrupt:\n",
    "        print(write_file(links))\n",
    "        \n",
    "        \n",
    "# get_ulrs_list()\n",
    "html = get_html('https://auto.ru/moskva/cars/all/')\n",
    "get_models_list(html.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2c77789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link 3731, active_link 0\n",
      "ERROR. Status code: 404\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Составляем датасет из ссылок на авто\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# функция генератор\n",
    "def generator(path):\n",
    "    # открываем файл\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        while True:\n",
    "            # читаем одну строку - ссылку\n",
    "            text = f.readline()\n",
    "            # Точка останова\n",
    "            if not text:\n",
    "                break\n",
    "            # с помощью yeild возвращаем ссылку\n",
    "            yield text[:-1]\n",
    "\n",
    "# функция парсинга страницы - принимает на вход ссылку\n",
    "def parsing_url_auto(url):\n",
    "    processed = False\n",
    "    # создаём словарь\n",
    "    spec = {'bodyType':'',\n",
    "            'brand':'',\n",
    "            'car_url':'',\n",
    "            'color':'',\n",
    "            'complectation_dict':'',\n",
    "            'description':'',\n",
    "            'engineDisplacement':'',\n",
    "            'enginePower':'',\n",
    "            'equipment_dict':'',\n",
    "            'fuelType':'',\n",
    "            'image':'',\n",
    "            'mileage':'',\n",
    "            'modelDate':'',\n",
    "            'model_info':'',\n",
    "            'model_name':'',\n",
    "            'name':'',\n",
    "            'numberOfDoors':'',\n",
    "            'parsing_unixtime':'',\n",
    "            'priceCurrency':'',\n",
    "            'productionDate':'',\n",
    "            'sell_id':'',\n",
    "            'super_gen':'',\n",
    "            'vehicleConfiguration':'',\n",
    "            'vehicleTransmission':'',\n",
    "            'vendor':'',\n",
    "            'Владельцы':'',\n",
    "            'Владение':'',\n",
    "            'ПТС':'',\n",
    "            'Привод':'',\n",
    "            'Руль':'',\n",
    "            'Состояние':'',\n",
    "            'Таможня':'',\n",
    "            'Комплектация':'',\n",
    "            'start_date':'',\n",
    "            'hidden':'',\n",
    "            'price':''}\n",
    "    # формируем гет запрос\n",
    "    for i in range(1): \n",
    "        \n",
    "        html = get_html(url)\n",
    "        # проверяем статус ответа\n",
    "        if not result_status(html):\n",
    "            time.sleep(0.5)\n",
    "            continue\n",
    "        processed = True\n",
    "        break\n",
    "    if processed:\n",
    "        return url\n",
    "    \n",
    "    # Парсим\n",
    "    # soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    # ищем нужные данные и пишем их в словарь\n",
    "    # \n",
    "    \n",
    "    # вовращаем готовый словарь\n",
    "\n",
    "# функция создания датафрейма - на вход принимает генератор\n",
    "    # количество обработанных ссылок\n",
    "    \n",
    "    # список для необработанных ссылок\n",
    "    # miss_url = []\n",
    "    # создаём dataframe\n",
    "    # в цикле\n",
    "        # получаем ссылку из генератора\n",
    "        # парсим данные по ссылке\n",
    "        # добавляем данные в ранее созданный dataframe\n",
    "    # пишем датафрайм в csv-файл\n",
    "try:\n",
    "    active_url = []\n",
    "    number = 1\n",
    "    for url in generator('Links.txt'):\n",
    "        clear_output()\n",
    "        print(f'Link {number}, active_link {len(active_url)}')\n",
    "        number += 1\n",
    "        if parsing_url_auto(url):\n",
    "            active_url.append(url)\n",
    "finally:\n",
    "    print(active_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b54b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
